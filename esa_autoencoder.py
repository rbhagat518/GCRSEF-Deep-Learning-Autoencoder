# -*- coding: utf-8 -*-
"""ESA AutoEncoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Om062gv2xIHB_pGOqHijNUoUB0dzOSo
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf

!unzip "/content/drive/Shareddrives/Course1ARam/ESAData/Active/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE_1991-2019-v05.2.zip"

!unzip "/content/drive/Shareddrives/Course1ARam/ESAData/Passive/ESACCI-SOILMOISTURE-L3S-SSMV-PASSIVE_1978-2019-v05.2.zip"

!pip install "xarray[complete]"
!pip install netCDF4
import numpy as np
import os
import matplotlib.pyplot as plt
import pickle
import datetime
import netCDF4 as nc4
!pip install esa_cci_sm
import os
from datetime import datetime
from esa_cci_sm.interface import CCI_SM_025Img,CCI_SM_025Ds
import numpy.testing as nptest
import cv2
import tensorflow as tf

import xarray as xr

ds_disk = xr.open_dataset("/content/active/2011/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-20110104000000-fv05.2.nc")
ds_disk

values = ds_disk["sm"]
values.data.shape
values.data = x.reshape(1,720,1440)
values[:500, ::3, ::3].plot()

print(np.amax(x))

values.to_netcdf('360-autoencoded20110104000000.nc')

np.amax(x)
#for i in range(720):
 # for j in range(1440):
  #  value = x[0][i][j] 
   # if value < 0:
      #x[0][i][j] = np.nan
#-9998.999997286988
for id in range(720):
  for y in range(1440):
    x[id][y] = x[id][y]*100

"""# Data Testing

Import Python Libraries
Install netCDF4

Store and convert netCDF4 into array
"""

#@title

with nc4.Dataset( "/content/active/1991/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-19910807000000-fv05.2.nc", mode='r') as ncfid:
     lat    = ncfid.variables['lat'][:]
     lon    = ncfid.variables['lon'][:]
     time = ncfid.variables['time'][:]

     # Print variable information
     print (time)

#@title
with nc4.Dataset("/content/active/2009/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-20090103000000-fv05.2.nc", mode='r') as ncfid:
     for name, var in ncfid.variables.items():
         print("{}:".format(name))
         print("{:>20}: {}".format("Dimension", var.dimensions))
         print("{:>20}: {}".format("Dimension value", var.shape))
         print("{:>20}: {}".format("Type", var.dtype))
         for attr in var.ncattrs():
             print("{:>20}: {}".format(attr, var.getncattr(attr)))

# read several parameters
parameter = ['sm']
# the class is initialized with the exact filename.

img = CCI_SM_025Img("/content/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-19910831000000-fv05.2.nc", parameter=parameter)
# reading returns an image object which contains a data dictionary
# with one array per parameter. The returned data is a global 0.25 degree
# image/array.
image = img.read()
array = img.read().data.get('sm')
print(array.shape)
grid1 = array


import cv2
res = cv2.resize(array, dsize=(1440, 720), interpolation=cv2.INTER_CUBIC)
count = 0
percent = []
for x in range(0, 720, 120):
  for y in range (0, 1440,120):
    count = 0
    for i in range(x, x+120):
      for j in range (y, y + 120):
        if (res[i][j]> -9999):
          count = count + 1
    percentage = count/14400
    percent.append(percentage)
print('percentage covered')
print(percent)

print("median")
median = np.median(percent)
print(median)

new_list = []
for x in range(0, 720, 120):
  for y in range (0, 1440,120):
    count = 0
    for i in range(x, x+120):
      for j in range (y, y + 120):
        if (res[i][j]> -9999):
          count = count + 1
    percentage = count/14400
    if (percentage > median):
      new_list.append([percentage, y, x])
print(new_list)

sm_good_list = []
for a in new_list:
  x = a[2]
  y = a[1]
  sm_good_list.append([res[x:x+120,y:y+120]])

print((sm_good_list[1][0]))

a = np.asarray(sm_good_list)
print(a.shape)
q,w,e,r = a.shape
print(q)

b = []
for i in range(q):
  b.append(np.asarray(a[i][0]))
y = np.asarray(b)
print(y[0][0])
y[0][0] = monthOfESACCIFile('/content/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-19910831000000-fv05.2.nc')
print(y[0][0])
print(y.shape)

c = []
for i in range(q):
  c.append(np.asarray(b[2][i]))
z = np.asarray(c)
print(z.shape)

import matplotlib.pyplot as plt
import tensorflow as tf

for i in range(36):
  plt.imshow((y[i]))
  plt.show()


plt.imshow(res)
plt.show()
print(type(res))

print(image)
print(image.data)

nc_file = "/content/active/2019/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-20190214000000-fv05.2.nc"

# Open the netCDF file and read surface air temperature
with nc4.Dataset(nc_file,'r') as ncid:
     lons      = ncid.variables['lon'][:] # longitude grid points
     lats      = ncid.variables['lat'][:] # latitude grid points
     soil_moisture = ncid.variables['sm'][:]

!apt-get install libproj-dev proj-data proj-bin
!apt-get install libgeos-dev
!pip install cython
!pip install cartopy

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import cm

import cartopy
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter

fig = plt.figure(figsize=(9, 5))
map_projection = ccrs.PlateCarree()
ax = fig.add_subplot(1, 1, 1, projection=map_projection)

im = ax.contourf(lons, lats, soil_moisture[0,:,:], transform=map_projection)
ax.coastlines()

ax.set_xticks(np.linspace(-180, 180, 5), crs=map_projection)
ax.set_yticks(np.linspace(-90, 90, 5), crs=map_projection)
lon_formatter = LongitudeFormatter(zero_direction_label=True)
lat_formatter = LatitudeFormatter()
ax.xaxis.set_major_formatter(lon_formatter)
ax.yaxis.set_major_formatter(lat_formatter)

cb = fig.colorbar(im, orientation='vertical', shrink=0.75)

ax.set_global()
plt.show()

#@title
with nc4.Dataset( "/content/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-19910831000000-fv05.2.nc", mode='r') as ncfid:

  time = ncfid.variables['time'][:]
print(time/365)

"""# Loading/Preparing Data"""

def monthOfESACCIFile(fname):
  a = fname.index("000000-")
  month = fname[a-4:a-2]
  return (int(month))

#@title
def toDatasetESACCIFile(fname):
  parameter = ['sm']
  img = CCI_SM_025Img(fname, parameter=parameter)
  array = img.read().data.get('sm')
  res = cv2.resize(array, dsize=(1440, 720), interpolation=cv2.INTER_CUBIC)
  count = 0
  percent = []
  for x in range(0, 720, 120):
    for y in range (0, 1440,120):
      count = 0
      for i in range(x, x+120):
        for j in range (y, y + 120):
          if (res[i][j]> -9999):
            count = count + 1
      percentage = count/14400
      percent.append(percentage)
  median = np.median(percent)
  new_list = []
  for x in range(0, 720, 120):
    for y in range (0, 1440,120):
      count = 0
      for i in range(x, x+120):
        for j in range (y, y + 120):
          if (res[i][j]> -9999):
            count = count + 1
      percentage = count/14400
      if (percentage > 0.08):
        new_list.append([percentage, y, x])
  sm_good_list = []
  for a in new_list:
    x = a[2]
    y = a[1]
    arr = res[x:x+120,y:y+120]
    arr[0][0] = month
    sm_good_list.append([arr])
  a = np.asarray(sm_good_list)
  print(a.shape)
  if (a.size > 0):
    q,w,e,r = a.shape
    masterDataset = []
    for i in range(q):
      masterDataset.append(np.asarray(np.asarray(a)[i][0]))
    z = np.asarray(masterDataset)
    z[0][0] = monthOfESACCIFile(fname)
    print(fname+" added")
    print(z.shape)
    return(z)
  else:
    print('unable to add'+fname)

masterDataset = []

toDatasetESACCIFileOptimized("/content/active/1997/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-19970103000000-fv05.2.nc")

print(np.asarray(masterDataset))

plt.imshow(np.asarray(masterDataset[0]))
plt.show()

print(data.shape)

def toDatasetESACCIFileOptimized(fname):
  parameter = ['sm']
  img = CCI_SM_025Img(fname, parameter=parameter)
  array = img.read().data.get('sm')
  res = cv2.resize(array, dsize=(1440, 720), interpolation=cv2.INTER_CUBIC)
  count = 0
  threshold_size = 720
  percent = []
  sm_good_list = []
  for x in range(0, 720, threshold_size):
    for y in range (0, 1440,threshold_size):
      arr = res[x:x+threshold_size,y:y+threshold_size]
      sm_good_list.append([arr])
  a = np.asarray(sm_good_list)
  print(a.shape)
  if (a.size > 0):
    q,w,e,r = a.shape
    for i in range(q):
      masterDataset.append(np.asarray(np.asarray(a)[i][0]))
  else:
    print('unable to add '+fname)

def SeventoDatasetESACCIFileOptimized(fname):
  parameter = ['sm']
  img = CCI_SM_025Img(fname, parameter=parameter)
  array = img.read().data.get('sm')
  res = cv2.resize(array, dsize=(1440, 720), interpolation=cv2.INTER_CUBIC)
  count = 0
  threshold_size = 720
  percent = []
  sm_good_list = []
  arr0 = res[0:720,0:720]
  arr1 = res[0:720,720:1440]
  sm_good_list.append([arr0])
  sm_good_list.append([arr1])
  a = np.asarray(sm_good_list)
  print(a.shape)
  if (a.size > 0):
    q,w,e,r = a.shape
    for i in range(q):
      masterDataset.append(np.asarray(np.asarray(a)[i][0]))
  else:
    print('unable to add '+fname)

#all active file reading
for i in range(1980,2020):
  for x in os.listdir("/content/passive/"+str(i)):
    SeventoDatasetESACCIFileOptimized("/content/passive/"+str(i)+"/"+x)
    print(np.asarray(masterDataset).shape)
  print("i =")
  print(i)
print("completed")



with open('720ThresholdArray.npy', 'wb') as validation_set:
    np.save(validation_set, np.asarray(masterDataset))

#1991 active file test
i=1994
for x in os.listdir("/content/active/"+str(i)):
  toDatasetESACCIFileOptimized("/content/active/"+str(i)+"/"+x)
  print(np.asarray(masterDataset).shape)
print("i =")
print(i)
print("completed")
with open('array_1991to1994.npy', 'wb') as validation_set:
    np.save(validation_set, np.asarray(masterDataset))

print(np.asarray(masterDataset).shape)

#individual file reader
toDatasetESACCIFile("/content/active/1991/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-19910906000000-fv05.2.nc")

print(monthOfESACCIFile("/content/active/1991/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-19910805000000-fv05.2.nc"))

count = 0
for x in os.listdir("/content/active/1991"):
  count += 1
print(count)

#Total number of images selected for training

total_num_images = 2000

#Select 2000 samples
img_database = np.array(img_database[9:total_num_images])

with open('/content/drive/Shareddrives/Course1ARam/Training_Form_Data/TrainingForm360Threshold_2015-2019.npy', 'rb') as validation_set:
    a = np.load(validation_set)
print(a[0])

data = np.fromfile("/content/drive/Shareddrives/Course1ARam/Masterdataset_91to98.npy")

"""# Autoencoder"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import os
import matplotlib
import matplotlib.pyplot as plt
import zipfile
import cv2
import time
from keras.layers import Conv2D, Input, Dense, Dropout, MaxPool2D, UpSampling2D
from keras.models import Model
# %matplotlib inline

def FormatArray(array):
  sample, y, x = array.shape
  for i in range(sample):
    for j in range(0,240):
      for k in range(0,240):
        array[i][j][k] += 9999

with open('/content/drive/Shareddrives/Course1ARam/Arrays/360ThresholdArray_2015-2019.npy', 'rb') as validation_set:
    a = np.load(validation_set)
FormatArray(a)
with open('TrainingForm360Threshold_2015-2019.npy', 'wb') as validation_set:
    np.save(validation_set, a)

with open('/content/drive/Shareddrives/Course1ARam/120autoencoder/360ThresholdArray_2006-2014.npy', 'rb') as validation_set:
    a = np.load(validation_set)

with open('/content/drive/Shareddrives/Course1ARam/Arrays/360ThresholdArray_1998-2005.npy', 'rb') as validation_set:
    b = np.load(validation_set)

print(a[0])

print(a.shape)

print(b.shape)

FormatArray(b)
with open('TrainingForm240Threshold_1999-2007.npy', 'wb') as validation_set:
    np.save(validation_set, b)

c = np.concatenate((a,b))

with open('360Threshold1998-2014.npy', 'wb') as validation_set:
    np.save(validation_set, c)

print(c.shape)

#@title
with open('array_1991to1994.npy', 'wb') as validation_set:
    np.save(validation_set, np.asarray(masterDataset))

#@title
with open('/content/drive/Shareddrives/Course1ARam/Arrays/DATEFIXEDarray_1999-2007.npy','rb') as validation_set:
  a = np.load(validation_set)

#@title
FormatArray(a)
print(a[0][0])
with open('TrainingFormData1999to2007.npy', 'wb') as validation_set:
    np.save(validation_set, a)

#@title
with open('TrainingFormData1991to1998.npy', 'wb') as validation_set:
    np.save(validation_set, a)

#@title
with open('/content/drive/Shareddrives/Course1ARam/Arrays/DATEFIXEDarray_1999-2007.npy','rb') as validation_set:
  b = np.load(validation_set)

plt.imshow((c[0]))
plt.show()

plt.imshow()

print(np.amax(autoencoder_ready_data))

"""Start of Autoencoder"""

with open('/content/drive/Shareddrives/Course1ARam/Training_Form_Data/TrainingForm120Threshold_1991to2007.npy','rb') as validation_set:
  autoencoder_ready_data = np.load(validation_set)

with open('/content/drive/Shareddrives/Course1ARam/120autoencoder/240train.npy','rb') as validation_set:
  train = np.load(validation_set)

print(np.amax(autoencoder_ready_data))

autoencoder_ready_data.shape

with open('testPart1.npy', 'wb') as validation_set:
    np.save(validation_set, train)

#Total number of images selected for training
total_sm_training = 60000
box_dimension = 120
max_value = np.amax(autoencoder_ready_data)
max = 13866.735

#Select samples
train = np.array(autoencoder_ready_data[0:total_sm_training])
train = train.reshape([-1,120,120,1])/max_value
print(train.shape)
test = np.array(autoencoder_ready_data[total_sm_training:total_sm_training+10000])
test = test.reshape([-1,120,120,1])/max_value
print(test.shape)
train_validation = np.array(autoencoder_ready_data[0:total_sm_training])
train_validation = train_validation.reshape([-1,box_dimension,box_dimension,1])/max_value
print(train_validation.shape)
test_validation = np.array(autoencoder_ready_data[total_sm_training:total_sm_training+10000])
test_validation = test_validation.reshape([-1,box_dimension,box_dimension,1])/max_value
print(test_validation.shape)

train = train.reshape([-1,360,360,1])/13866.735
print(train.shape)

with open('train_gaps.npy', 'wb') as validation_set:
    np.save(validation_set, train_gaps)

test = np.array(autoencoder_ready_data[45000:45000+9000])

def bigGaps(train_array):
  q,w,e,r = train_array.shape
  for i in range(q):
    for z in range(random.randrange(23)+20):
        boxX = random.randrange(15) + 10
        boxY = random.randrange(15) + 10
        x = random.randrange(w-boxX)
        y = random.randrange(e-boxY)
        for j in range(x, (x+boxX)):
            for k in range (y, (y+boxY)):
                train_array[i][j][k] = 0. 
  return (train_array)

test = test.reshape([-1,240,240,1])/13866.735
print(test.shape)

train_validation = np.array(autoencoder_ready_data[0:total_sm_training])
train_validation = train_validation.reshape([-1,box_dimension,box_dimension,1])/max_value
print(train_validation.shape)

FormatArray(np.asarray(masterDataset))
print(np.asarray(masterDataset[0][1]))

test_validation = np.array(autoencoder_ready_data[total_sm_training:total_sm_training+9000])
test_validation = test_validation.reshape([-1,box_dimension,box_dimension,1])/max_value
print(test_validation.shape)

import random
def gapInESAFile(train_array, number_gaps):
  q,w,e,r = train_array.shape
  for i in range(q):
    for z in range(number_gaps):
      x = random.randrange(w)
      y = random.randrange(e)
      train_array[i][x][y] = 0.
  return (train_array)

#@title
import random
def gapInSample(train_array, number_gaps):
  w,e,r = train_array.shape
  for i in range(number_gaps):
    x = random.randrange(120)
    y = random.randrange(120)
    train_array[x][y][0] = 0.
  return (train_array)

# Adding gaps to data
import random
train_gaps =  bigGaps(train)
test_gaps = bigGaps(test)

plt.imshow(train_gaps[2].reshape(120,120))
plt.show

plt.imshow(train_gaps[2].reshape(box_dimension,box_dimension))
plt.show

plt.imshow(test_validation[3].reshape(box_dimension,box_dimension))
plt.show

plt.imshow(test_gaps[3].reshape(box_dimension,box_dimension))
plt.show

with open('/content/drive/Shareddrives/Course1ARam/120autoencoder/240train.npy','rb') as validation_set:
  train_validation = np.load(validation_set)

with open('/content/drive/Shareddrives/Course1ARam/240test.npy','rb') as validation_set:
  test_validation = np.load(validation_set)

with open('/content/drive/Shareddrives/Course1ARam/train_gaps.npy','rb') as validation_set:
  train_gaps = np.load(validation_set)

with open('/content/drive/Shareddrives/Course1ARam/test_gaps.npy','rb') as validation_set:
  test_gaps = np.load(validation_set)

#@title
noise = 0.3
train_noise = train + noise * np.random.normal(0, 1, size=train.shape)
test_noise = test + noise * np.random.normal(0, 1, size=test.shape)

train_noise = np.clip(train_noise, 0, 1)
test_noise = np.clip(test_noise, 0, 1)
print(train_noise.shape)
print(test_noise.shape)

# Encoder 
inputs = Input(shape=(120,120,1))

x = Conv2D(16, 3, activation='relu', padding='same')(inputs)
x = MaxPool2D()(x)
x = Dropout(0.3)(x)
x = Conv2D(16, 3, activation='relu', padding='same')(x)
x = MaxPool2D()(x)
x = Dropout(0.3)(x)
x = Conv2D(16, 3, activation='relu', padding='same')(x)
encoded = MaxPool2D()(x)

# Decoder

x = Conv2D(16, 3, activation='relu', padding='same')(encoded)
x = UpSampling2D()(x)
x = Dropout(0.3)(x)
x = Conv2D(16, 3, activation='relu', padding='same')(x)
x = UpSampling2D()(x)
x = Dropout(0.3)(x)
x = Conv2D(16, 3, activation='relu', padding='same')(x)
x = UpSampling2D()(x)
decoded = Conv2D(1, 3, activation='sigmoid', padding='same')(x)

autoencoder = Model(inputs, decoded)
autoencoder.compile(optimizer='rmsprop', loss='mse')

autoencoder.summary()

epochs = 50
batch_size = 256

history = autoencoder.fit(train_gaps,
                train_validation,
                epochs=epochs,
                batch_size=batch_size,
                shuffle=True,
                validation_data=(test_gaps, test_validation)
               )

autoencoder.save('120_bottleneck_autoencoder')

import h5py
autoencoder.save('my_model.h5')

# Defining Figure
f = plt.figure(figsize=(10,7))
f.add_subplot()

#Adding Subplot
plt.plot(history.epoch, history.history['loss'], label = "loss") # Loss curve for training set
plt.plot(history.epoch, history.history['val_loss'], label = "val_loss") # Loss curve for validation set

plt.title("120 bottleneck Autoencoder Loss_curve Loss Curve",fontsize=18)
plt.xlabel("Epochs",fontsize=15)
plt.ylabel("Loss",fontsize=15)
plt.grid(alpha=0.3)
plt.legend()
plt.savefig("120 bottleneck Autoencoder Loss_curve.png")
plt.show()

test_autoencoder = autoencoder.predict(b) # predict

plt.imshow(test_validation[35].reshape([120,120]))
plt.show()

plt.imshow(test_gaps[35].reshape([120,120]))
plt.show()

plt.imshow(test_autoencoder[1].reshape([120,120]))#sample 18 aint so good compared to og
plt.show()

# Select few random test images
num_imgs = 16
rand = np.random.randint(1, 1000)
no_gapped = test_validation[rand:rand+num_imgs]
gapped = test_gaps[rand:rand+num_imgs] # slicing
predicted = autoencoder.predict(gapped) # predict

plt.imshow(no_gapped[1].reshape(120,120))
plt.show

plt.imshow(gapped[1].reshape(120,120))
plt.show

plt.imshow(predicted[1].reshape(120,120))
plt.show

# Visualize test images with their denoised images

rows = 2 # defining no. of rows in figure
cols = 8 # defining no. of colums in figure
box = 120
f = plt.figure(figsize=(3*cols,4*rows*3)) # defining a figure 

for i in range(rows):
  for j in range(cols): 
    f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration
    plt.imshow(no_gapped[i*cols + j].reshape([box,box])) 
    plt.axis("off")
  for j in range(cols): 
    f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration
    plt.imshow(gapped[i*cols + j].reshape([box,box])) 
    plt.axis("off")
  for j in range(cols): 
    f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration
    plt.imshow(predicted[i*cols + j].reshape([box,box])) 
    plt.axis("off")
        

        
f.suptitle("120 bottleneck Autoencoder Results",fontsize=18)
plt.savefig("autoencoder 120 results.png")

plt.show()

# Visualize test images with their denoised images

rows = 3 # defining no. of rows in figure
cols = 8 # defining no. of colums in figure
box = 120
f = plt.figure(figsize=(2*cols,2*rows*2)) # defining a figure 

for i in range(1):
  for j in range(cols): 
    f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration
    plt.imshow(gapped[i*cols + j].reshape([box,box])) 
    plt.axis("off")
  for j in range(cols): 
    f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration
    plt.imshow(predicted[i*cols + j].reshape([box,box])) 
    plt.axis("off")


        
f.suptitle("120 bottleneck Autoencoder Results",fontsize=18)
plt.savefig("autoencoder 120 results.png")

plt.show()

for j in range(cols): 
  f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration
  plt.imshow(no_gapped[i*cols + j].reshape([box,box])) 
  plt.axis("off")  

plt.show()

# Visualize test images with their denoised images

rows = 1 # defining no. of rows in figure
cols = 8 # defining no. of colums in figure
box = 120
f = plt.figure(figsize=(2*cols,2*rows*2)) # defining a figure 

for j in range(cols): 
  f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration
  plt.imshow(no_gapped[i*cols + j].reshape([box,box])) 
  plt.axis("off")
for j in range(cols): 
  f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration
  plt.imshow(gapped[i*cols + j].reshape([box,box])) 
  plt.axis("off")        

        
f.suptitle("120 bottleneck Autoencoder Results",fontsize=18)
plt.savefig("autoencoder 120 results.png")

plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Generate random data
data = np.random.rand(100)

# Plot in different subplots
fig, (ax1, ax2) = plt.subplots(1, 2)
ax1.plot(data)

ax2.plot(data)

ax1.plot(data+1)

plt.show()

"""

```
```

# Clipping Images together"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import os
import matplotlib
import matplotlib.pyplot as plt
import zipfile
import cv2
import time
from keras.layers import Conv2D, Input, Dense, Dropout, MaxPool2D, UpSampling2D
from keras.models import Model
# %matplotlib inline

def monthOfESACCIFile(fname):
  a = fname.index("000000-")
  month = fname[a-4:a-2]
  return (int(month))

# Recreate the exact same model, including its weights and the optimizer
autoencoder_240 = tf.keras.models.load_model('/content/drive/Shareddrives/Course1ARam/240N15Bottleneck/240boxgaps15N_autoencoder')

# Show the model architecture
autoencoder_240.summary()

# Recreate the exact same model, including its weights and the optimizer
autoencoder_120 = tf.keras.models.load_model('/content/drive/Shareddrives/Course1ARam/120_bottleneck_autoencoder')

# Show the model architecture
autoencoder_120.summary()

# Recreate the exact same model, including its weights and the optimizer
autoencoder_360 = tf.keras.models.load_model('/content/drive/Shareddrives/Course1ARam/360boxgaps45N_autoencoder.h5')

# Show the model architecture
autoencoder_360.summary()

masterDataset = []

def digestFile(fname):
  parameter = ['sm']
  img = CCI_SM_025Img(fname, parameter=parameter)
  array = img.read().data.get('sm')
  res = cv2.resize(array, dsize=(1440, 720), interpolation=cv2.INTER_CUBIC)
  count = 0
  threshold_size = 240
  percent = []
  sm_good_list = []
  for x in range(0, 720, threshold_size):
    for y in range (0, 1440,threshold_size):
        arr = res[x:x+threshold_size,y:y+threshold_size]
        sm_good_list.append([arr])
  a = np.asarray(sm_good_list)
  print(a.shape)
  if (a.size > 0):
    q,w,e,r = a.shape
    for i in range(q):
      masterDataset.append(np.asarray(np.asarray(a)[i][0]))
  else:
    print('unable to add '+fname)

digestFile("/content/active/2011/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-20110104000000-fv05.2.nc")
print(np.asarray(masterDataset).shape)



data = np.asarray(masterDataset)
print(np.amax(data))

print(data[1])

sample,x,y = data.shape
for j in range(sample):
  for k in range(x):
    for l in range(y):
      if (np.isnan(data[j][k][l]) == True):
        data[j][k][l]=-9999.
print(np.amax(data))

print(data[0])

#plt.imshow(data[9])
#plt.show
hs = -9999
for x in range(240):
  for y in range(240):
    if data[9][x][y] > hs:
      hs = data[9][x][y]
print(hs)
np.amin(data)

def FormatArray(array):
  sample, y, x = array.shape
  for i in range(sample):
    for j in range(0,y):
      for k in range(0,x):
        array[i][j][k] += 9999

def deFormatArray(array):
  sample, y, x, o = array.shape
  for i in range(sample):
    for j in range(0,y):
      for k in range(0,x):
        array[i][j][k] -= 9999

FormatArray(data)
print(np.amax(data))

plt.imshow(data[2].reshape(360,360))
plt.show()

data = data.reshape([-1,360,360,1])/(np.amax(data))

print(np.amax(data))

predicted_360 = autoencoder_360.predict(data)

print(np.amax(predicted_360))

predicted_120 = autoencoder_120.predict(data)

deFormatArray(predicted_360)

predicted_360 = predicted_360*100

print(predicted_360[0])
print(np.amax(predicted_360))

deFormatArray(predicted_120)

plt.imshow(predicted_360[0].reshape(360,360))
plt.show()

plt.imshow(data[0].reshape(240,240))
plt.show()

print(predicted.shape)

"""Start of Clipping"""

x = np.zeros((720,1440))
print(x.shape)
print(x[0])
plt.imshow(x)
plt.show()

y = np.zeros((720,1440))
print(y.shape)
print(y[0])
plt.imshow(y)
plt.show()

z = np.zeros((720,1440))
plt.imshow(z)
plt.show()

a = np.zeros((720,1440))
plt.imshow(a)
plt.show()

u = 0
box = 360
for n in range(0,720,box):
  for m in range(0,1440,box):
    x[n:n+box,m:m+box] = predicted_360[u].reshape(box,box)     
    u = u+1

x[0:240,0:240] = predicted[0].reshape(240,240)     
x[0:240,240:480] = predicted[1].reshape(240,240)

u = 0
box = 360
for n in range(0,720,box):
  for m in range(0,1440,box):
    y[n:n+box,m:m+box] = data[u].reshape(box,box)     
    u = u+1

f = plt.figure() # defining a figure 
plt.imshow(y)
f.suptitle("Normal Results",fontsize=18)
plt.axis("off")
plt.show()

f = plt.figure() # defining a figure 
plt.imshow(x)
f.suptitle("360-Autoencoded Results",fontsize=18)
plt.axis("off")
plt.show()

r,f = x.shape
for q in range(r):
  for c in range(f):
    x[q][c] -= 9999.

plt.imshow(x)
plt.axis("off")
plt.show()

x.shape

with open('/content/drive/Shareddrives/Course1ARam/240test.npy', 'rb') as validation_set:
    a = np.load(validation_set)

with open('/content/drive/Shareddrives/Course1ARam/test_gaps.npy', 'rb') as validation_set:
    b = np.load(validation_set)

print(a.shape)
print(b.shape)

predicted_240 = autoencoder_240.predict(b)

# Visualize test images with their denoised images

rows = 2 # defining no. of rows in figure
cols = 8 # defining no. of colums in figure
box = 240
f = plt.figure(figsize=(2*cols,2*rows*2)) # defining a figure 

for i in range(rows):
    for j in range(cols): 
        f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration
        plt.imshow(b[i*cols + j].reshape([box,box])) 
        plt.axis("off")
        
    for j in range(cols): 
        f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration
        plt.imshow(a[i*cols + j].reshape([box,box])) 
        plt.axis("off")
        
f.suptitle("120 bottleneck Autoencoder Results",fontsize=18)
plt.savefig("autoencoder 120 results.png")

plt.show()

"""Percentage"""

import xarray as xr

# Recreate the exact same model, including its weights and the optimizer
masterDataset = []
autoencoder_240 = tf.keras.models.load_model('/content/drive/Shareddrives/Course1ARam/240N15Bottleneck/240boxgaps15N_autoencoder')
fill_percentages = []
def processFile(file):
  digestFile(file)
  data = np.asarray(masterDataset)
  print(data.shape)
  sample,x,y = data.shape
  for j in range(sample):
    for k in range(x):
      for l in range(y):
        if (np.isnan(data[j][k][l]) == True):
          data[j][k][l]=-9999
  FormatArray(data)
  print(np.amax(data))
  print(data.shape)
  data = data.reshape([-1,240,240,1])/(np.amax(data))
  predicted = autoencoder_240.predict(data)
  deFormatArray(predicted)
  x = np.zeros((720,1440))
  y = np.zeros((720,1440))
  u = 0
  box = 240
  for n in range(0,720,box):
    for m in range(0,1440,box):
      x[n:n+box,m:m+box] = data[u].reshape(box,box)
      y[n:n+box,m:m+box] = predicted[u].reshape(box,box)
      u = u+1
  return x, y

x, y = processFile('/content/active/2011/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-20110104000000-fv05.2.nc') 
#Doesn't matter what file you open with ds_disk--any file is fine because the data will be overwriten
ds_disk = xr.open_dataset("/content/active/2011/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-20110104000000-fv05.2.nc")
valuesNormal = ds_disk["sm"]
valuesNormal.data = x.reshape(1,720,1440)
valuesNormal.to_netcdf('Normal_240_trail12.nc')
valuesPredicted = ds_disk["sm"]
valuesPredicted.data = y.reshape(1,720,1440)

valuesPredicted.to_netcdf('Predicted_240_trail12.nc')

masterDataset = []
digestFile('/content/active/2011/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-20110104000000-fv05.2.nc')
data = np.asarray(masterDataset)
sample,x,y = data.shape
for j in range(sample):
  for k in range(x):
    for l in range(y):
      if (np.isnan(data[j][k][l]) == True):
        data[j][k][l]=-9999
FormatArray(data)
print(np.amax(data))
data = data.reshape([-1,240,240,1])

ds_disk = xr.open_dataset("/content/active/2011/ESACCI-SOILMOISTURE-L3S-SSMS-ACTIVE-20110104000000-fv05.2.nc")
valuesNormal = ds_disk["sm"]
normal = np.asarray(valuesNormal)
print(normal.shape)
plt.imshow(normal.reshape(720, 1440))
plt.show()